---
layout: slide
title: ""
---

# History of computer vision in 4 slides #3

<div markdown="1" style="font-size:2vw">

- *Deep*: 7 hidden “weight” layers
- *Learned*: all feature extractors initialized at white Gaussian noise and learned from the data
- *Entirely supervised*
- *More data* = good

- *Trained with stochastic gradient descent on two NVIDIA GPUs for about a week*
- *650,000 neurons, 60,000,000 parameters, [630,000,000 connections](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/tutorials/tut6_slides.pdf){: .pleaseletmeclickonthislink}*
- *Final feature layer: 4096-dimensional*

</div>

![Alexnet](assets/pics/old-imgs/AlexNet.png){: .slideimage height="50%" width="50%"}


<div markdown="1" class="slideimage" style="text-align: unset; position: inherit; left: 80%; top: 50%;">

![neuron](assets/pics/old-imgs/neuron.png)

</div>

<figcaption class="figcaption" markdown="1">

Credits: [Alex Krizhevsky - ImageNet Classification with Deep Convolutional Neural Networks](https://web.archive.org/web/20201024130347/http://image-net.org/challenges/LSVRC/2012/supervision.pdf){: .pleaseletmeclickonthislink}

</figcaption>

